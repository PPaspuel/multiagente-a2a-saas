"""
Herramientas personalizadas para el agente almacenador.
VERSI√ìN MEJORADA:
- Soporte para an√°lisis en ResponseFormatter
- Nuevas funciones de formateo HTML
"""

import json
import logging
import re
from typing import Dict, List, Any, Optional
from pathlib import Path
import PyPDF2
import io

logger = logging.getLogger(__name__)


class PDFProcessor:
    """
    Clase para procesar archivos PDF y extraer su contenido de texto.
    """
    
    @staticmethod
    def extract_text_from_pdf(pdf_content: bytes) -> str:
        """
        Extrae todo el texto de un archivo PDF.
        
        Args:
            pdf_content: Contenido del PDF en bytes
            
        Returns:
            str: Texto extra√≠do del PDF, p√°gina por p√°gina
        """
        try:
            pdf_file = io.BytesIO(pdf_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text_pages = []
            
            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                
                if text.strip():
                    text_pages.append(f"--- P√°gina {page_num + 1} ---\n{text}")
                    
            full_text = "\n\n".join(text_pages)
            
            logger.info(f"‚úì PDF procesado exitosamente: {len(pdf_reader.pages)} p√°ginas")
            return full_text
            
        except Exception as e:
            logger.error(f"‚úó Error al extraer texto del PDF: {str(e)}")
            raise ValueError(f"No se pudo procesar el PDF: {str(e)}")
    
    
    @staticmethod
    def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """
        Divide el texto en fragmentos (chunks) por caracteres.
        NOTA: Se mantiene como fallback. Se recomienda usar semantic_chunking().
        
        Args:
            text: Texto completo a fragmentar
            chunk_size: Tama√±o m√°ximo de cada fragmento en caracteres
            overlap: Cantidad de caracteres que se solapan entre fragmentos
            
        Returns:
            List[str]: Lista de fragmentos de texto
        """
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            
            if chunk.strip():
                chunks.append(chunk)
            
            start = end - overlap
            
        logger.info(f"‚úì Texto fragmentado en {len(chunks)} chunks (m√©todo por caracteres)")
        return chunks

    @staticmethod
    def semantic_chunking(text: str, similarity_threshold: float = 0.5) -> List[str]:
        """
        Divide el texto en fragmentos sem√°nticamente coherentes usando
        sentence-transformers y similitud coseno entre oraciones consecutivas.

        Los chunks se forman agrupando oraciones mientras su similitud sem√°ntica
        supera el umbral. Cuando la similitud cae, se abre un nuevo chunk.
        Tambi√©n filtra fragmentos que son solo t√≠tulos, numeraciones o texto vac√≠o.

        Args:
            text: Texto completo a fragmentar (idealmente texto extra√≠do de PDF).
            similarity_threshold: Umbral de similitud coseno entre [0, 1].
                - Valores altos (ej: 0.7-0.9): chunks m√°s peque√±os y tem√°ticamente homog√©neos.
                - Valores bajos (ej: 0.3-0.5): chunks m√°s grandes con m√°s variedad tem√°tica.
                Se recomienda comenzar con 0.5 y ajustar seg√∫n el dominio del documento.

        Returns:
            List[str]: Lista de fragmentos sem√°nticamente coherentes y limpios.
                       Los chunks vac√≠os, t√≠tulos cortos y numeraciones se descartan.

        Raises:
            ImportError: Si sentence-transformers o nltk no est√°n instalados.
        """
        try:
            from sentence_transformers import SentenceTransformer, util
            import nltk
        except ImportError as e:
            logger.error(
                "‚ùå Dependencias faltantes para chunking sem√°ntico. "
                "Instala con: pip install sentence-transformers nltk"
            )
            raise ImportError(
                "sentence-transformers y nltk son requeridos para semantic_chunking. "
                f"Error original: {e}"
            )

        # Descargar tokenizador de oraciones si no est√° disponible
        try:
            nltk.data.find('tokenizers/punkt_tab')
        except LookupError:
            logger.info("üì• Descargando recursos NLTK (punkt_tab)...")
            nltk.download('punkt_tab', quiet=True)

        # Cargar modelo ligero y eficiente para generar embeddings
        logger.info("ü§ñ Cargando modelo de embeddings sem√°nticos (all-MiniLM-L6-v2)...")
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

        # Tokenizar el texto en oraciones individuales
        sentences = nltk.sent_tokenize(text)

        if not sentences:
            logger.warning("‚ö†Ô∏è No se encontraron oraciones en el texto")
            return []

        if len(sentences) == 1:
            return [sentences[0].strip()] if sentences[0].strip() else []

        # Generar embeddings para todas las oraciones en un solo paso (eficiente)
        logger.info(f"üî¢ Generando embeddings para {len(sentences)} oraciones...")
        embeddings = model.encode(sentences, show_progress_bar=False)

        # Calcular similitud coseno entre oraciones consecutivas
        similarities = [
            util.cos_sim(embeddings[i], embeddings[i + 1]).item()
            for i in range(len(embeddings) - 1)
        ]

        # Agrupar oraciones en chunks seg√∫n el umbral de similitud
        # Cuando la similitud cae por debajo del umbral ‚Üí ruptura sem√°ntica ‚Üí nuevo chunk
        raw_chunks = []
        current_chunk = [sentences[0]]

        for i, score in enumerate(similarities):
            if score < similarity_threshold:
                raw_chunks.append(" ".join(current_chunk))
                current_chunk = []
            current_chunk.append(sentences[i + 1])

        if current_chunk:
            raw_chunks.append(" ".join(current_chunk))

        # ‚îÄ‚îÄ Limpieza y filtrado de chunks ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Elimina prefijos de numeraci√≥n al inicio: "1.", "I.", "2.1.", "III." etc.
        prefix_pattern = re.compile(
            r"^\s*(?:(?:[IVXLCDMivxlcdm]+|\d+)\.(?:\d+\.)*\s*)"
        )

        def is_title_or_noise(t: str, min_length: int = 15) -> bool:
            """
            Retorna True si el fragmento es probablemente ruido o un t√≠tulo sin
            contenido informativo suficiente para almacenar en la base vectorial.
            """
            t = t.strip()
            if not t:
                return True
            if len(t) < min_length:
                return True
            alpha_chars = sum(c.isalpha() for c in t)
            if alpha_chars == 0:
                return True  # Solo s√≠mbolos/n√∫meros
            upper_chars = sum(c.isupper() for c in t)
            if upper_chars / len(t) > 0.75:
                return True  # Probable encabezado en may√∫sculas
            if re.fullmatch(
                r"^\s*(?:SECTION|ARTICLE|CLAUSE|CAP√çTULO|SECCI√ìN|ART√çCULO)\s+"
                r"(?:[IVXLCDM]+|\d+)\s*$",
                t, re.IGNORECASE
            ):
                return True
            return False

        filtered_chunks = []
        for chunk in raw_chunks:
            cleaned = prefix_pattern.sub("", chunk, count=1).strip()
            if is_title_or_noise(cleaned):
                logger.debug(f"üóëÔ∏è Chunk descartado (ruido/t√≠tulo): '{cleaned[:60]}'")
                continue
            filtered_chunks.append(cleaned)

        logger.info(
            f"‚úì Chunking sem√°ntico: {len(raw_chunks)} chunks crudos ‚Üí "
            f"{len(filtered_chunks)} chunks v√°lidos almacenables"
        )
        return filtered_chunks


class ResponseFormatter:
    """
    Clase para formatear respuestas en JSON y HTML estructurado.
    """
    
    @staticmethod
    def format_success_response(
        operation: str,
        data: Dict[str, Any],
        message: str = "Operaci√≥n exitosa"
    ) -> str:
        """
        Crea una respuesta JSON exitosa.
        
        Args:
            operation: Tipo de operaci√≥n realizada
            data: Datos relevantes de la operaci√≥n
            message: Mensaje descriptivo
            
        Returns:
            str: JSON formateado como string
        """
        response = {
            "status": "success",
            "operation": operation,
            "message": message,
            "data": data,
            "timestamp": None
        }
        
        return json.dumps(response, ensure_ascii=False, indent=2)
    
    
    @staticmethod
    def format_error_response(
        operation: str,
        error_message: str,
        error_type: str = "ProcessingError"
    ) -> str:
        """
        Crea una respuesta JSON de error.
        
        Args:
            operation: Tipo de operaci√≥n que fall√≥
            error_message: Descripci√≥n del error
            error_type: Tipo de error
            
        Returns:
            str: JSON formateado como string
        """
        response = {
            "status": "error",
            "operation": operation,
            "error": {
                "type": error_type,
                "message": error_message
            }
        }
        
        return json.dumps(response, ensure_ascii=False, indent=2)
    
    
    @staticmethod
    def format_storage_response(
        num_chunks: int,
        total_characters: int,
        collection_name: str,
        document_id: Optional[str] = None,
        was_updated: bool = False
    ) -> str:
        """
        Formatea la respuesta despu√©s de almacenar en Qdrant.
        
        Args:
            num_chunks: N√∫mero de fragmentos almacenados
            total_characters: Total de caracteres procesados
            collection_name: Nombre de la colecci√≥n en Qdrant
            document_id: ID del documento almacenado
            was_updated: Si fue una actualizaci√≥n de documento existente
            
        Returns:
            str: JSON con informaci√≥n del almacenamiento
        """
        data = {
            "chunks_stored": num_chunks,
            "total_characters": total_characters,
            "collection": collection_name,
            "document_id": document_id,
            "was_updated": was_updated,
            "storage_location": "qdrant_local"
        }
        
        action = "actualizado" if was_updated else "almacenado"
        message = f"PDF {action} exitosamente en {num_chunks} fragmentos"
        
        return ResponseFormatter.format_success_response(
            operation="store_pdf",
            data=data,
            message=message
        )
    
    
    @staticmethod
    def render_storage_response_html(response: dict) -> str:
        """
        Renderiza la respuesta de almacenamiento en HTML.
        
        Args:
            response: Diccionario con la respuesta
            
        Returns:
            str: HTML formateado
        """
        was_updated = response.get("data", {}).get("was_updated", False)
        icon = "üîÑ" if was_updated else "üìÑ"
        action = "Actualizaci√≥n" if was_updated else "Almacenamiento"
        
        return f"""
            <h3>{icon} Reporte de {action.lower()}</h3>

            <p><b>Estado:</b> {response.get("status")}</p>
            <p><b>Operaci√≥n:</b> {response.get("operation")}</p>

            <p>{response.get("message")}</p>

            <h3>üìä Detalles del proceso</h3>
            <ul>
                <li><b>Nombre del documento:</b> {response["data"].get("filename", "N/A")}</li>
                <li><b>Fragmentos almacenados:</b> {response["data"]["chunks_stored"]}</li>
                <li><b>Total de caracteres:</b> {response["data"]["total_characters"]}</li>
                <li><b>Colecci√≥n:</b> {response["data"]["collection"]}</li>
                <li><b>Tipo:</b> {"Actualizaci√≥n de documento existente" if was_updated else "Nuevo documento"}</li>
                <li>
                    <b>Document ID:</b>
                    <code>{response["data"].get("document_id", "N/A")}</code>
                </li>
            </ul>
        """
    
    
    @staticmethod
    def render_analysis_response_html(
        analysis_list: List[Dict[str, Any]],
        document_id: Optional[str] = None
    ) -> str:
        """
        Renderiza una lista de an√°lisis en HTML.
        
        Args:
            analysis_list: Lista de an√°lisis recuperados
            document_id: ID del documento (opcional)
            
        Returns:
            str: HTML formateado
        """
        if not analysis_list:
            return """
            <h3>üì≠ No se encontraron an√°lisis</h3>
            <p>No hay an√°lisis almacenados para este documento.</p>
            """
        
        html_parts = [
            f"<h3>üìä An√°lisis encontrados ({len(analysis_list)})</h3>"
        ]
        
        if document_id:
            html_parts.append(f"<p><b>Document ID:</b> {document_id[:16]}...</p>")
        
        for i, analysis in enumerate(analysis_list, 1):
            html_parts.append(f"""
            <div>
                <h3>üîç An√°lisis #{i}</h3>
                <p><b>ID:</b> {analysis['analysis_id'][:16]}...</p>
                <p><b>Documento:</b> {analysis['document_id'][:16]}...</p>
                <p><b>Tipo:</b> {analysis['analysis_type']}</p>
                <p><b>Fecha:</b> {analysis['created_at']}</p>
                
                <h3>üìù Contenido:</h3>
                <div>
                    {analysis['analysis_content']}
                </div>
            </div>
            """)
        
        return "\n".join(html_parts)


# FUNCIONES DE UTILIDAD
def validate_pdf_content(content: bytes) -> bool:
    """
    Valida que el contenido sea un PDF v√°lido.
    
    Args:
        content: Bytes del archivo a validar
        
    Returns:
        bool: True si es un PDF v√°lido
    """
    try:
        pdf_signature = b'%PDF'
        return content.startswith(pdf_signature)
    except Exception:
        return False


def get_pdf_metadata(pdf_content: bytes) -> Dict[str, Any]:
    """
    Extrae metadatos del PDF.
    
    Args:
        pdf_content: Contenido del PDF en bytes
        
    Returns:
        Dict con metadatos del PDF
    """
    try:
        pdf_file = io.BytesIO(pdf_content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        metadata = {
            "num_pages": len(pdf_reader.pages),
            "has_text": False
        }
        
        if len(pdf_reader.pages) > 0:
            first_page_text = pdf_reader.pages[0].extract_text()
            metadata["has_text"] = bool(first_page_text.strip())
        
        if pdf_reader.metadata:
            metadata.update({
                "title": pdf_reader.metadata.get('/Title', 'N/A'),
                "author": pdf_reader.metadata.get('/Author', 'N/A'),
                "subject": pdf_reader.metadata.get('/Subject', 'N/A'),
            })
        
        return metadata
        
    except Exception as e:
        logger.error(f"Error al extraer metadatos: {str(e)}")
        return {"num_pages": 0, "has_text": False, "error": str(e)}


def extract_document_id_from_text(text: str) -> Optional[str]:
    """
    Extrae un UUID (document_id) del texto usando expresiones regulares.
    
    Args:
        text: Texto donde buscar el UUID
        
    Returns:
        str: UUID encontrado o None
    """
    import re
    uuid_pattern = r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}'
    match = re.search(uuid_pattern, text, re.IGNORECASE)
    return match.group(0) if match else None